What is Bias in a statistical model?;Error from approximating a real-world problem with a simpler model.<br>High bias models are too simple and may underfit.<br>Defined as the difference between the true function and the average expected fit: $\text{Bias}(\hat{f})^2 = E\left[\left(f - E[\hat{f}]\right)^2\right]$.
What is Variance in a statistical model?;Measures how much a model's fit would change if trained on a different dataset.<br>Caused by a model being too complex and learning from noise in the training data.<br>Leads to overfitting.<br>Defined as the variability between different model instantiations: $\text{Var}(\hat{f}) = E\left[\left(E[\hat{f}] - \hat{f}\right)^2\right]$.
What is k-fold Cross-Validation (CV)?;A resampling method to estimate a model's test error.<br><b>Process:</b><br>1. Randomly divide the data into K folds (e.g., K=5 or 10).<br>2. Train the model on K-1 folds.<br>3. Validate on the one held-out fold.<br>4. Repeat K times and average the results.
Properties of Leave-One-Out CV (LOOCV);A type of k-fold CV where K equals the number of observations, n.<br><b>Low Bias:</b> The model is trained on almost the entire dataset (n-1 samples), so it has less bias than k-fold CV.<br><br><b>High Variance:</b> The training sets for each fold are nearly identical, so their outputs are highly correlated, which inflates the variance of the final estimate.
What is the core method of the Bootstrap?;A resampling technique that samples the original data <b>with replacement</b>.<br>Each new "bootstrap dataset" has the same size (n) as the original, but some observations may be duplicated while others are left out.
What is the primary use of the Bootstrap?;To assess the uncertainty, standard errors, or confidence intervals of an estimator.<br>It provides accurate measures of both the bias and variance of an estimator, which is useful when statistical assumptions are violated.
What is the "Curse of Dimensionality"?;In high dimensions, data becomes very sparse, and the distance between observations tends to be large.<br>The complexity of functions can grow exponentially with dimensionality, requiring an exponential increase in data for accurate fitting.<br>Leads to a high risk of overfitting and poor generalization.
What is Ridge Regression?;A shrinkage method that fits a model with all 'p' predictors.<br>Adds an L2 penalty to the loss function: $RSS + \lambda\sum_{j=1}^{p} \beta_j^2$.<br>Shrinks coefficients towards zero but <b>not</b> exactly to zero.<br>Effective for handling collinearity by keeping all related variables.
What is Lasso Regression?;A shrinkage method that can perform variable selection.<br>Adds an L1 penalty to the loss function: $RSS + \lambda\sum_{j=1}^{p} |\beta_j|$.<br>The L1 penalty can shrink coefficients <b>exactly to zero.</b><br>Useful when you believe many predictors are irrelevant.
Why must you standardize predictors for Ridge & Lasso?;The penalty is based on the size of the coefficients.<br>If predictors are on different scales, the penalty is applied unfairly to those with naturally larger coefficients.<br>Standardization ensures all predictors are on a comparable scale before the penalty is applied.
What is Principal Component Analysis (PCA)?;A dimension reduction technique.<br>It transforms a set of correlated variables into a smaller set of new, uncorrelated variables called <b>principal components</b>.<br>The first principal component is the linear combination of predictors that captures the largest variance.
What is Principal Component Regression (PCR)?;A two-step regression approach using dimension reduction.<br>1. <b>PCA:</b> First, create principal components from the predictors.<br>2. <b>Regression:</b> Then, fit a standard linear regression model using these components as the new predictors.
What is the main problem with a high-degree Polynomial for regression?;It performs a global fit, describing the whole range of the predictor with a single function.<br>This can cause the function to behave erratically and "flap wildly" in the data-sparse tails, leading to high variance.
What is a Spline?;A function built by connecting piecewise low-degree polynomials (e.g., cubic).<br>The connection points are called <b>knots</b>.<br>The pieces are connected smoothly, ensuring continuity of the function and its derivatives, which creates a much more stable fit than a single high-degree polynomial.
What is a Smoothing Spline?;A function g(x) that minimizes a combination of the Residual Sum of Squares (RSS) and a roughness penalty.<br>The penalty term, $\lambda\int g''(t)^2 dt$, penalizes "bendiness".<br>It is a natural cubic spline with knots at every unique data point.
How is the flexibility of a Smoothing Spline controlled?;Flexibility is controlled by the smoothing parameter, λ.<br><b>High λ:</b> The roughness penalty dominates, and the function becomes a straight line (high bias).<br><b>Low λ:</b> The RSS term dominates, and the function becomes very "wiggly" to fit the data perfectly (high variance).
What is a Generalized Additive Model (GAM)?;Extends the GLM by replacing linear terms with flexible, non-linear functions ($f_j$).<br>Formulation: $g(E[y_i]) = \beta_0 + \sum f_j(x_{ij})$.<br>It is additive, so the effect of each predictor can be examined independently, preserving some interpretability.
What is Bagging?;Stands for <b>Bootstrap Aggregating</b>.<br><b>Process:</b> Create many models by training on bootstrap samples (data sampled with replacement) and then average their predictions.<br><b>Goal:</b> To reduce the variance of an unstable model like a decision tree.
What is the key modification in a Random Forest?;It's an improvement on bagging for decision trees that decorrelates the trees.<br><b>Key Idea:</b> At each split in a tree, it considers only a <b>random subset of predictors</b> as split candidates, instead of all of them.
Why do Random Forests decorrelate trees?;By restricting the choice of predictors at each split, it prevents one or two very strong predictors from being chosen at the top of every tree.<br>This forces the ensemble to use a wider variety of predictors, making the resulting trees less similar to each other.<br>Averaging less correlated models reduces variance more effectively.
What is Boosting?;A sequential or progressive ensemble method that learns "slowly".<br><b>Process:</b><br>1. Fit a weak model (e.g., a small tree) to the data.<br>2. Fit the next model on the <b>residuals</b> (errors) of the previous one.<br>3. Repeat, with each model focusing on the mistakes of its predecessor.<br>Can overfit if too many trees are used.